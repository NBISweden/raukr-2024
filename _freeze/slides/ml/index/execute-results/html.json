{
  "hash": "7861cc6752df52ba1cf4614410cfd616",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mathematical statistics and machine learning in R\"\nauthor: \"Nikolay Oskolkov\"\nimage: \"assets/featured.jpg\"\nformat:\n  revealjs:\n    fig-align: center\n---\n\n\n## Packages {visibility=\"hidden\"}\n\n\n::: {.cell}\n\n:::\n\n\n## Biological data are high dimensional\n\n![](assets/high_dimensional_data.png){width=\"100%\" .center}\n\n## Types of data analysis\n\n![](assets/AmountOfData.png){width=\"150%\" .center}\n\n\n\n## Some peculiarities of Frequentist statistics\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n- based on Maximum Likelihood principle\n- focus ~~too much~~ on summary statistics\n\n::: {.fragment}\n$$\\rm{L}\\,(\\,x_i \\,|\\, \\mu,\\sigma^2\\,) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp^{\\displaystyle -\\frac{\\sum\\limits_{i=1}^N (x_i-\\mu)^2}{2\\sigma^2}}$$\n:::\n\n::: {.fragment}\n$$\\frac{\\partial \\rm{L}\\,(\\,x_i \\,|\\, \\mu,\\sigma^2\\,)}{\\partial\\mu} = 0; \\,\\, \\frac{\\partial \\rm{L}\\,(\\,x_i \\,|\\, \\mu,\\sigma^2\\,)}{\\partial\\sigma^2} = 0$$\n:::\n\n::: {.fragment}\n$$\\mu = \\frac{1}{N}\\sum_{i=0}^N x_i \\,\\,\\rm{-}\\,\\rm{mean \\, estimator}$$\n\n$$\\sigma^2 = \\frac{1}{N}\\sum_{i=0}^N (x_i-\\mu)^2 \\,\\,\\rm{-}\\,\\rm{variance \\, estimator}$$\n:::\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\n![](assets/Anscombes_quartet.png){width=\"150%\" .center}\nSummary statistics do not always reasonbly describe data (example: Anscombes quartet)\n:::\n\n:::\n\n::::\n\n\n## Frequentist statistics: focus ~~to much~~ on p-values\n\n:::: {.columns}\n\n::: {.column width=\"55%\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nFC<-1.02; x_mean<-5; x_sd<-1; N_vector<-seq(from=100,to=10000,by=100); pvalue_t<-vector(); pvalue_lm<-vector()\nfor(N in N_vector)\n{\n  x1 <- rnorm(N, x_mean, x_sd); x2 <- rnorm(N, x_mean*FC, x_sd)\n  t_test_res<-t.test(x1, x2); pvalue_t <- append(pvalue_t, t_test_res$p.value)\n\n  x <- rnorm(N, 0, 1); y <- 0.1*x+2*rnorm(N, 0, 1)\n  lm_res <- summary(lm(y~x)); pvalue_lm <- append(pvalue_lm, lm_res$coefficients[2,4])\n}\npar(mfrow=c(2,2)); par(mar = c(5, 5, 1, 1))\nboxplot(x1, x2, names=c(\"X1\",\"X2\"), ylab=\"Value\", col=\"darkred\"); mtext(\"Fold change FC = 1.02\")\nplot(pvalue_t~N_vector,type='o',xlab=\"N\",ylab=\"p-value\",col=\"darkgreen\"); mtext(\"P-value of two-group t-test\")\nplot(y~x, xlab=\"X\", ylab=\"Y\"); abline(lm(y~x), col=\"blue\", lwd=2); mtext(\"Y = 0.1*X + 2*rnorm(N, 0, 1)\")\nplot(pvalue_lm~N_vector,type='o',xlab=\"N\",ylab=\"p-value\",col=\"darkgreen\"); mtext(\"P-value of linear regression\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"45%\"}\n::: {.fragment}\n![](assets/Pvalue.png){width=\"100%\" .center}\n\nQuestionable whether p-value is a best metric for ranking features (biomarkers)\n:::\n:::\n\n::::\n\n## Frequentist statistics struggles with high-dimensional data\n\n:::: {.columns}\n\n::: {.column width=\"32%\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nn <- 20 # number of samples\np <- 2  # number of features / dimensions\nY <- rnorm(n)\nX <- matrix(rnorm(n * p), n, p)\nsummary(lm(Y ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0522 -0.6380  0.1451  0.3911  1.8829 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.14950    0.22949   0.651    0.523\nX1          -0.09405    0.28245  -0.333    0.743\nX2          -0.11919    0.24486  -0.487    0.633\n\nResidual standard error: 1.017 on 17 degrees of freedom\nMultiple R-squared:  0.02204,\tAdjusted R-squared:  -0.09301 \nF-statistic: 0.1916 on 2 and 17 DF,  p-value: 0.8274\n```\n\n\n:::\n:::\n\n\n::: {.fragment}\nGoing to higher dimensions &rarr;\n:::\n\n:::\n\n::: {.column width=\"35%\"}\n\n::: {.fragment}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nn <- 20 # number of samples\np <- 10 # number of features / dimensions\nY <- rnorm(n)\nX <- matrix(rnorm(n * p), n, p)\nsummary(lm(Y ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0255 -0.4320  0.1056  0.4493  1.0617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.54916    0.26472   2.075   0.0679 .\nX1           0.30013    0.21690   1.384   0.1998  \nX2           0.68053    0.27693   2.457   0.0363 *\nX3          -0.10675    0.26010  -0.410   0.6911  \nX4          -0.21367    0.33690  -0.634   0.5417  \nX5          -0.19123    0.31881  -0.600   0.5634  \nX6           0.81074    0.25221   3.214   0.0106 *\nX7           0.09634    0.24143   0.399   0.6992  \nX8          -0.29864    0.19004  -1.571   0.1505  \nX9          -0.78175    0.35408  -2.208   0.0546 .\nX10          0.83736    0.36936   2.267   0.0496 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8692 on 9 degrees of freedom\nMultiple R-squared:  0.6592,\tAdjusted R-squared:  0.2805 \nF-statistic: 1.741 on 10 and 9 DF,  p-value: 0.2089\n```\n\n\n:::\n:::\n\n:::\n::: {.fragment}\nGoing to even higher dimensions &rarr;\n:::\n:::\n\n::: {.column width=\"33%\"}\n\n::: {.fragment}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nn <- 20 # number of samples\np <- 20 # number of features / dimensions\nY <- rnorm(n)\nX <- matrix(rnorm(n * p), n, p)\nsummary(lm(Y ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\nALL 20 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  1.34889        NaN     NaN      NaN\nX1           0.66218        NaN     NaN      NaN\nX2           0.76212        NaN     NaN      NaN\nX3          -1.35033        NaN     NaN      NaN\nX4          -0.57487        NaN     NaN      NaN\nX5           0.02142        NaN     NaN      NaN\nX6           0.40290        NaN     NaN      NaN\nX7           0.03313        NaN     NaN      NaN\nX8          -0.31983        NaN     NaN      NaN\nX9          -0.92833        NaN     NaN      NaN\nX10          0.18091        NaN     NaN      NaN\nX11         -1.37618        NaN     NaN      NaN\nX12          2.11438        NaN     NaN      NaN\nX13         -1.75103        NaN     NaN      NaN\nX14         -1.55073        NaN     NaN      NaN\nX15          0.01112        NaN     NaN      NaN\nX16         -0.50943        NaN     NaN      NaN\nX17         -0.47576        NaN     NaN      NaN\nX18          0.31793        NaN     NaN      NaN\nX19          1.43615        NaN     NaN      NaN\nX20               NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:    NaN \nF-statistic:   NaN on 19 and 0 DF,  p-value: NA\n```\n\n\n:::\n:::\n\n\n:::\n:::\n::::\n\n\n## Equidistant points in high dimensions\n\n\n::: {.cell .small}\n\n```{.r .small .cell-code}\nn <- 1000; p <- c(2, 32, 512); pair_dist <- list()\nfor(i in 1:length(p)) {\n  X <- matrix(rnorm(n * p[i]), n, p[i])\n  pair_dist[[i]] <- as.vector(dist(X));\n  pair_dist[[i]] <- pair_dist[[i]] / max(pair_dist[[i]])\n}\n```\n:::\n\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/Hist_Plot-1.png){width=960}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n::: {.fragment}\n\n<br/><br/>\n\n- Data points in high dimensions:\n<br/><br/>\n\n  - move **away** from each other\n<br/><br/>\n\n  - become **equidistant** and similar\n<br/><br/>\n\n- Impossible to see differences between cases and controls\n\n:::\n\n:::\n::::\n\n\n## Regularizations: LASSO\n\n$$Y = \\beta_1X_1+\\beta_2X_2+\\epsilon$$\n\n. . .\n\n$$\\textrm{OLS} = (Y-\\beta_1X_1-\\beta_2X_2)^2$$\n\n. . .\n\n$$\\textrm{Penalized OLS} = (Y-\\beta_1X_1-\\beta_2X_2)^2 + \\lambda(|\\beta_1|+|\\beta_2|)$$\n\n. . .\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n![](assets/Kfold_CrossVal.jpg){width=\"90%\"}\n\n:::\n::: {.column width=\"40%\"}\n::: {.fragment}\n\n![](assets/CV_lambda.jpg){width=\"100%\"}\n\n:::\n:::\n::::\n\n## Regularizations are priors in Bayesian statistics\n\n$$\\small Y = \\beta_1X_1+\\beta_2X_2+\\epsilon; \\,\\,\\, Y \\sim N(\\,\\beta_1X_1+\\beta_2X_2, \\sigma^2\\,) \\equiv \\rm{L}\\,(\\,\\rm{Y} \\,|\\, \\beta_1,\\beta_2\\,)$$\n\n- **Maximum Likelihood** principle: maximize probability to observe data given parameters:\n$$\\small \\rm{L}\\,(\\,\\rm{Y} \\,|\\, \\beta_1,\\beta_2\\,) = \\frac{1}{\\sqrt{2\\pi\\sigma²}} \\exp^{\\displaystyle -\\frac{(Y-\\beta_1X_1-\\beta_2X_2)^2}{2\\sigma²}}$$\n\n::: {.fragment}\n- **Bayes theorem**: maximize posterior probability of observing parameters given data:\n$$\\small \\rm{Posterior}(\\rm{params} \\,|\\, \\rm{data})=\\frac{L(\\rm{data} \\,|\\, \\rm{params})*\\rm{Prior}(\\rm{params})}{\\int{L(\\rm{data} \\,|\\, \\rm{params})*\\rm{Prior}(\\rm{params}) \\, d(\\rm{params})}}$$\n:::\n::: {.fragment}\n$$\\small \\rm{Posterior}(\\,\\beta_1,\\beta_2\\,|\\, \\rm{Y}\\,) \\sim \\rm{L}\\,(\\,\\rm{Y} \\,|\\,\\beta_1,\\beta_2\\,)*\\rm{Prior}(\\beta_1,\\beta_2) \\sim \\exp^{-\\frac{(Y-\\beta_1X_1-\\beta_2X_2)^2}{2\\sigma²}}*\\exp^{-\\lambda(|\\beta_1|+|\\beta_2|)} \\\\\n\\small -\\log{\\left[\\rm{Posterior}(\\, \\beta_1,\\beta_2 \\,|\\, \\rm{Y}\\,)\\right]} \\sim (Y-\\beta_1X_1-\\beta_2X_2)^2 + \\lambda(|\\beta_1|+|\\beta_2|)$$\n:::\n\n## Markov Chain Monte Carlo (MCMC): introduction\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n::: {.fragment}\n\n- Integration via Monte Carlo sampling\n\n![](assets/MC.png){width=\"100%\"}\n\n:::\n::: {.fragment}\n\n$$\\small I = 2\\int\\limits_2^4{x dx}=2\\frac{x^2}{2} \\Big|_2^4 = 16 - 4 = 12$$\n\n:::\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nf <- function(x){return(2*x)}; a <- 2; b <- 4; N <- 10000; count <- 0\nx <- seq(from = a, to = b, by = (b-a) / N); y_max <- max(f(x))\nfor(i in 1:N)\n{\n  x_sample <- runif(1, a, b); y_sample <- runif(1, 0, y_max)\n  if(y_sample <= f(x_sample)){count <- count + 1}\n}\npaste0(\"Integral by Monte Carlo: I = \", (count / N) * (b - a) * y_max)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Integral by Monte Carlo: I = 11.9248\"\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\n\n- Markov Chain Monte Carlo (MCMC)\n\n![](assets/mcmc.png){width=\"80%\"}\n\n::: {.fragment}\n$$\\small \\rm{Hastings \\,\\, ratio} = \\frac{\\rm{Posterior}\\,(\\,\\rm{params_{next}} \\,|\\, \\rm{data}\\,)}{\\rm{Posterior}\\,(\\,\\rm{params_{previous}} \\,|\\, \\rm{data}\\,)}$$\n:::\n\n::: {.fragment}\n- If Hastings ratio > *u* [0, 1], then **accept**, else **reject**\n<br/><br/>\n- Hastings ratio does not contain the intractable integral from Bayes theorem\n:::\n\n:::\n\n:::\n\n::::\n\n## Markov Chain Monte Carlo (MCMC) from scratch in R\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- Example from population genetics\n\n![](assets/genotypes.png){width=\"70%\"}\n\n::: {.fragment}\n$$\\small L(n \\, | \\, f) = \\prod_g{\\left[ {2\\choose g} f^g (1-f)^{2-g}  \\right]^{n_g}}$$\n:::\n\n::: {.fragment}\n$$\\small \\frac{\\partial \\log\\left[L(n | f)\\right]}{\\partial f} = 0 \\, \\Rightarrow \\hat{f}=\\frac{n_1+2n_2}{2(n_0+n_1+n_2)}$$\n:::\n\n::: {.fragment}\n$$\\small \\rm{Prior}(f, \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} f^{\\alpha-1} (1-f)^{\\beta-1}$$\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nN <- 100; n <- c(25, 50, 25) # Observed genotype data for N individuals\nf_MLE <- (n[2] + 2*n[3]) / (2 * sum(n)) # MLE of allele frequency\n\n# Define log-likelihood function (log-binomial distribution)\nLL <- function(n, f){return((n[2] + 2*n[3])*log(f) + (n[2] + 2*n[1])*log(1-f))}\n# Define log-prior function (log-beta distribution)\nLP <- function(f, alpha, beta){return(dbeta(f, alpha, beta, log = TRUE))}\n\n# Run MCMC Metropolis - Hastings sampler\nf_poster <- vector(); alpha <- 0.5; beta <- 0.5; f_cur <- 0.1 # initialization\nfor(i in 1:1000)\n{\n  f_next <- abs(rnorm(1, f_cur, 0.1)) # make random step for allele frequency\n  \n  LL_cur <- LL(n, f_cur); LL_next <- LL(n, f_next)\n  LP_cur <- LP(f_cur, alpha, beta); LP_next <- LP(f_next, alpha, beta)\n  hastings_ratio <- LL_next + LP_next - LL_cur - LP_cur\n  \n  if(hastings_ratio > log(runif(1))){f_cur <- f_next}; f_poster[i] <- f_cur\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n:::\n\n:::\n\n::::\n\n## Moving from statistics to machine learning\n\n:::: {.columns}\n::: {.column width=\"45%\"}\n\n- Statistics is more **analytical** (pen & paper)\n\n::: {.fragment}\n$$\\rm{L}\\,(\\,x_i \\,|\\, \\mu,\\sigma^2\\,) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp^{\\displaystyle -\\frac{\\sum\\limits_{i=1}^N (x_i-\\mu)^2}{2\\sigma^2}}$$\n:::\n\n::: {.fragment}\n$$\\frac{\\partial \\rm{L}\\,(\\,x_i \\,|\\, \\mu,\\sigma^2\\,)}{\\partial\\mu} = 0; \\,\\, \\frac{\\partial \\rm{L}\\,(\\,x_i \\,|\\, \\mu,\\sigma^2\\,)}{\\partial\\sigma^2} = 0$$\n:::\n\n::: {.fragment}\n$$\\mu = \\frac{1}{N}\\sum_{i=0}^N x_i \\,\\,\\rm{-}\\,\\rm{mean \\, estimator}$$\n\n$$\\sigma^2 = \\frac{1}{N}\\sum_{i=0}^N (x_i-\\mu)^2 \\,\\,\\rm{-}\\,\\rm{variance \\, estimator}$$\n:::\n\n:::\n\n\n::: {.column width=\"55%\"}\n\n::: {.fragment}\n- Machine Learning is more **algorithmic** (ex. K-means)\n:::\n\n::: {.fragment}\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nK = 3; set.seed(123); c = X[sample(1:dim(X)[1],K),]; par(mfrow=c(2,2),mai=c(0.8,1,0,0))\nplot(X, xlab = \"X\", ylab = \"Y\", pch = 19); points(c, col = \"blue\", cex = 3, pch = 19)\nfor(t in 1:3)\n{\n  l <- vector()\n  for(i in 1:dim(X)[1])\n  {\n    d <- vector(); for(j in 1:K){d[j] <- sqrt((X[i,1]-c[j,1])^2 + (X[i,2]-c[j,2])^2)} \n    l[i] <- which.min(d)\n  }\n  plot(X, xlab=\"X\", ylab=\"Y\", col=l, pch=19); points(c, col=\"blue\", cex=3, pch=19)\n  s = list(); for(i in unique(l)){s[[i]] <- colMeans(X[l==i,])}; c = Reduce(\"rbind\", s)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){width=864}\n:::\n:::\n\n\n:::\n\n:::\n\n::::\n\n## Statistics vs. machine learning: prediction\n\n:::: {.columns}\n\n::: {.column width=\"36%\"}\n::: {.fragment}\n![](assets/stats.jpg)\n:::\n:::\n\n::: {.column width=\"14%\"}\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\n![](assets/ml.jpg)\n:::\n\n:::\n\n::::\n\n\n## How does machine learning work?\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n![](assets/TrainTestSplit.jpg){width=\"100%\"}\n\n:::\n::: {.column width=\"40%\"}\n\nMachine Learning typically involves five basic steps:\n<br/><br/>\n1. Split data set into **train**, **validation** and **test** subsets\n<br/><br/>\n\n::: {.fragment}\n2. Fit the model on the train subset\n<br/><br/>\n:::\n\n::: {.fragment}\n3. Validate your model on the validation subset\n<br/><br/>\n:::\n\n::: {.fragment}\n4. Repeat train - validation split many times and tune **hyperparameters**\n<br/><br/>\n:::\n\n::: {.fragment}\n5. Test the accuracy of the optimized model on the test subset.\n:::\n\n:::\n::::\n\n## Toy example of machine learning\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nN <- 100\nx <- rnorm(N)\ny <- 2 * x + rnorm(N)\ndf <- data.frame(x, y)\nplot(y ~ x, data = df, col = \"blue\")\nlegend(\"topleft\", \"Data points\", fill = \"blue\", bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n\n::: {.cell}\n\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\ntrain <- df[sample(1:dim(df)[1], 0.7 * dim(df)[1]), ]\ntest <- df[!rownames(df) %in% rownames(train), ]\ndf$col <- ifelse(rownames(df) %in% rownames(test), \"red\", \"blue\")\nplot(y ~ x, data = df, col = df$col)\nlegend(\"topleft\", c(\"Train\",\"Test\"), fill=c(\"blue\",\"red\"), bty=\"n\")\nabline(lm(y ~ x, data = train), col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-19-1.png){width=768}\n:::\n:::\n\n\n:::\n\n:::\n::::\n\n## Toy example: model validation\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell .small}\n\n```{.r .small .cell-code}\ntest_predicted <- as.numeric(predict(lm(y ~ x, data = train), newdata = test))\nplot(test$y ~ test_predicted, ylab = \"True y\", xlab = \"Pred y\", col = \"red\")\nabline(lm(test$y ~ test_predicted), col = \"darkgreen\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-20-1.png){width=768}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n::: {.fragment}\n\n\n::: {.cell .small}\n\n```{.r .small .cell-code}\nsummary(lm(test$y ~ test_predicted))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = test$y ~ test_predicted)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80597 -0.78005  0.07636  0.52330  2.61924 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     0.02058    0.21588   0.095    0.925    \ntest_predicted  0.89953    0.08678  10.366 4.33e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.053 on 28 degrees of freedom\nMultiple R-squared:  0.7933,\tAdjusted R-squared:  0.7859 \nF-statistic: 107.4 on 1 and 28 DF,  p-value: 4.329e-11\n```\n\n\n:::\n:::\n\n\n<br/><br/>\nThus the model explains 79% of variation on the test subset.\n:::\n:::\n::::\n\n## From linear models to artificial neural networks (ANNs)\n\n:::: {.columns}\n\n::: {.column width=\"55%\"}\n- ANN: a mathematical function Y = f(X) with a special architecture\n\n::: {.fragment}\n- Can be **non-linear** depending on **activation function**\n<br/><br/>\n:::\n\n::: {.fragment}\n![](assets/ANN.jpg){width=\"100%\"}\n:::\n\n:::\n\n::: {.column width=\"45%\"}\n\n::: {.fragment}\n- Backward propagation (**gradient descent**) for minimizing error\n:::\n\n::: {.fragment}\n- Universal Approximation Theorem\n<br/><br/>\n:::\n\n::: {.fragment}\n![](assets/UAT.jpg){width=\"100%\"}\n:::\n\n:::\n::::\n\n## Gradient descent\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![](assets/GD.png){width=\"100%\"}\n\n$$y_i = \\alpha + \\beta x_i + \\epsilon, \\,\\, i = 1 \\ldots n$$\n\n$$E(\\alpha, \\beta) = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\alpha - \\beta x_i)^2$$\n\n:::\n\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\n$$\\hat{\\alpha}, \\hat{\\beta} = \\rm{argmin} \\,\\, E(\\alpha, \\beta)$$\n:::\n\n::: {.fragment}\n$$\\frac{\\partial E(\\alpha, \\beta)}{\\partial\\alpha} = -\\frac{2}{n}\\sum_{i=1}^n(y_i - \\alpha - \\beta x_i)$$\n\n$$\\frac{\\partial E(\\alpha, \\beta)}{\\partial\\beta} = -\\frac{2}{n}\\sum_{i=1}^n x_i(y_i - \\alpha - \\beta x_i)$$\n:::\n\n::: {.fragment}\nNumeric implementation of gradient descent:\n\n$$\\alpha_{i+1} = \\alpha_i - \\eta \\left. \\frac{\\partial E(\\alpha, \\beta)}{\\partial\\alpha} \\right\\vert_{\\alpha=\\alpha_i,\\beta=\\beta_i}$$\n\n$$\\beta_{i+1} = \\beta_i - \\eta \\left. \\frac{\\partial E(\\alpha, \\beta)}{\\partial\\beta} \\right\\vert_{\\alpha=\\alpha_i,\\beta=\\beta_i}$$\n:::\n\n:::\n\n::::\n\n## Coding gradient descent from scratch in R\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n::: {.cell}\n\n:::\n\n::: {.cell .small}\n\n```{.r .small .cell-code}\nn <- 100 # sample size\nx <- rnorm(n) # simulated expanatory variable\ny <- 3 + 2 * x + rnorm(n) # simulated response variable\nsummary(lm(y ~ x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9073 -0.6835 -0.0875  0.5806  3.2904 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.89720    0.09755   29.70   <2e-16 ***\nx            1.94753    0.10688   18.22   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9707 on 98 degrees of freedom\nMultiple R-squared:  0.7721,\tAdjusted R-squared:  0.7698 \nF-statistic:   332 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n<br/><br/>\nLet us now reconstruct the intercept and slope from gradient descent\n:::\n\n\n::: {.column width=\"55%\"}\n::: {.fragment}\n\n\n::: {.cell .small}\n\n```{.r .small .cell-code}\nalpha <- vector(); beta <- vector()\nE <- vector(); dEdalpha <- vector(); dEdbeta <- vector()\neta <- 0.01; alpha[1] <- 1; beta[1] <- 1 # initialize alpha and beta\nfor(i in 1:1000)\n{\n  E[i] <- (1/n) * sum((y - alpha[i] - beta[i] * x)^2)  \n  dEdalpha[i] <- - sum(2 * (y - alpha[i] - beta[i] * x)) / n\n  dEdbeta[i] <- - sum(2 * x * (y - alpha[i] - beta[i] * x)) / n\n  \n  alpha[i+1] <- alpha[i] - eta * dEdalpha[i]\n  beta[i+1] <- beta[i] - eta * dEdbeta[i]\n}\nprint(paste0(\"alpha = \", tail(alpha, 1),\", beta = \", tail(beta, 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"alpha = 2.89719694937354, beta = 1.94752837381973\"\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.fragment}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n## ANN from scratch in R: problem formulation\n\n:::: {.columns}\n\n::: {.column width=\"57%\"}\n\n![](assets/Problem.png)\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nd <- c(0, 0, 1, 1)  # true labels\nx1 <- c(0, 0, 1, 1) # input variable x1\nx2 <- c(0, 1, 0, 1) # input variable x2\n\ndata.frame(x1 = x1, x2 = x2, d = d)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| x1| x2|  d|\n|--:|--:|--:|\n|  0|  0|  0|\n|  0|  1|  0|\n|  1|  0|  1|\n|  1|  1|  1|\n\n</div>\n:::\n:::\n\n:::\n\n::: {.column width=\"43%\"}\n\n::: {.fragment}\n![](assets/ANN_Scheme.png)\n:::\n\n::: {.fragment}\n$$y(w_1,w_2)=\\phi(w_1x_1+w_2x_2)$$\n:::\n\n::: {.fragment}\n$$\\phi(s)=\\frac{1}{1+e^{\\displaystyle -s}} \\,\\,\\rm{-}\\,\\rm{sigmoid}$$\n:::\n\n::: {.fragment}\n$$\\phi^\\prime(s)=\\phi(s)\\left(1-\\phi(s)\\right)$$\n:::\n\n:::\n\n::::\n\n## ANN from scratch in R: implementation in code\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nphi <- function(x){return(1/(1 + exp(-x)))} # activation function\n\nmu <- 0.1; N_epochs <- 10000\nw1 <- 0.1; w2 <- 0.5; E <- vector()\nfor(epochs in 1:N_epochs)\n{\n  #Forward propagation\n  y <- phi(w1 * x1 + w2 * x2 - 3) # we use a fixed bias -3\n  \n  #Backward propagation\n  E[epochs] <- (1 / (2 * length(d))) * sum((d - y)^2)\n  dE_dw1 <- - (1 / length(d)) * sum((d - y) * y * (1 - y) * x1)\n  dE_dw2 <- - (1 / length(d)) * sum((d - y) * y * (1 - y) * x2)\n  w1 <- w1 - mu * dE_dw1\n  w2 <- w2 - mu * dE_dw2\n}\nplot(E ~ seq(1:N_epochs), xlab=\"Epochs\", ylab=\"Error\", col=\"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n$$E(w_1,w_2)=\\frac{1}{2N}\\sum_{i=1}^N\\left(d_i-y_i(w_1,w_2)\\right)^2$$\n:::\n\n::: {.fragment}\n$$w_{1,2}=w_{1,2}-\\mu\\frac{\\partial E(w_1,w_2)}{\\partial w_{1,2}}$$\n:::\n\n::: {.fragment}\n$$\\frac{\\partial E}{\\partial w_1} = -\\frac{1}{N}\\sum_{i=1}^N (d_i-y_i)*y_i*(1-y_i)*x_{1i}$$\n\n$$\\frac{\\partial E}{\\partial w_2} = -\\frac{1}{N}\\sum_{i=1}^N (d_i-y_i)*y_i*(1-y_i)*x_{2i}$$\n:::\n\n::: {.fragment}\n\n::: {.cell .small}\n\n```{.r .small .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04742587 0.05752359 0.95730271 0.96489475\n```\n\n\n:::\n:::\n\nWe nearly reconstruct true labels **d = (0, 0, 1, 1)**\n\n:::\n\n:::\n\n::::\n\n## Decision tree from scratch in R: problem formulation\n\n:::: {.columns}\n\n::: {.column width=\"48%\"}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nX<-data.frame(height=c(183,167,178,171),weight=c(78,73,85,67))\ny<-as.factor(c(\"Female\", \"Male\", \"Male\", \"Female\"))\ndata.frame(X, sex = y)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| height| weight|sex    |\n|------:|------:|:------|\n|    183|     78|Female |\n|    167|     73|Male   |\n|    178|     85|Male   |\n|    171|     67|Female |\n\n</div>\n:::\n:::\n\n\n::: {.fragment}\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nlibrary(\"rpart\"); library(\"rpart.plot\")\nfit<-rpart(y~height+weight,data=X,method=\"class\",minsplit=-1)\nrpart.plot(fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n:::\n\n:::\n\n::: {.column width=\"52%\"}\n\n::: {.fragment}\n- Let us visualize what the classifier has learnt\n:::\n\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\ncolor <- c(\"red\", \"blue\", \"blue\", \"red\")\nplot(height ~ weight, data = X, col = color, pch = 19, cex = 3)\nlegend(\"topleft\",c(\"Male\",\"Female\"),fill=c(\"blue\",\"red\"),inset=.02)\n\nabline(h = 169, lty = 2, col = \"darkgreen\", lwd = 1.5)\nabline(v = 82, lty = 2, col = \"darkgreen\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n## Decision tree from scratch in R: Gini index and split\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n![](assets/Gini.png){width=\"100%\"}\n\n::: {.fragment}\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\ngini <- function(x)\n{\n  return(1 - sum((table(x) / length(x))^2))\n}\ngini(c(1, 0, 1, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\ngini(c(1, 1, 0, 1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.32\n```\n\n\n:::\n:::\n\n:::\n\n:::\n\n::: {.column width=\"55%\"}\n\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nget_best_split <- function(X, y)\n{\n  mean_gini <- vector(); spl_vals <- vector(); spl_names <- vector()\n  for(j in colnames(X)) # for each variable in X data frame\n  {\n    spl <- vector() # vector of potential split candidates\n    sort_X <- X[order(X[, j]), ]; sort_y <- y[order(X[, j])] # sort by variable\n    for(i in 1:(dim(X)[1]-1)) # for each observation of variable in X data frame\n    {\n      spl[i] <- (sort_X[i, j] + sort_X[(i + 1), j]) / 2 # variable consecutive means\n      g1_y <- sort_y[sort_X[, j] > spl[i]] # take labels for group above split\n      g2_y <- sort_y[sort_X[, j] < spl[i]] # take labels for group below split\n      mean_gini <- append(mean_gini, (gini(g1_y) + gini(g2_y))/2) # two groups mean Gini\n      spl_vals <- append(spl_vals, spl[i])\n      spl_names <- append(spl_names, j)\n    }\n  }\n  min_spl_val <- spl_vals[mean_gini == min(mean_gini)][1] # get best split variable\n  min_spl_name <- spl_names[mean_gini == min(mean_gini)][1] # get best split value\n  sort_X <- X[order(X[, min_spl_name]), ] # sort X by best split variable\n  sort_y <- y[order(X[, min_spl_name])] # sort y by best split variable\n  g1_y <- sort_y[sort_X[, min_spl_name] > min_spl_val] # labels above best split\n  g2_y <- sort_y[sort_X[, min_spl_name] < min_spl_val] # labels below best split\n  if(gini(g1_y) == 0){sex <- paste0(\"Above: \", as.character(g1_y))}\n  else if(gini(g2_y) == 0){sex <- paste0(\"Below: \", as.character(g2_y))}\n  \n  return(list(spl_name = min_spl_name, spl_value = min_spl_val, sex = sex))\n}\nget_best_split(X, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$spl_name\n[1] \"height\"\n\n$spl_value\n[1] 169\n\n$sex\n[1] \"Below: Male\"\n```\n\n\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n## Decision tree from scratch in R: code implementation\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- After we have found the best split, let us check what group we can split further: **get_new_data** function\n\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\nget_new_data <- function(X, y)\n{\n  spl_name <- get_best_split(X, y)$spl_name\n  spl_val <- get_best_split(X, y)$spl_value\n  \n  # Sort X and y by the variable of the best split\n  sort_X <- X[order(X[, spl_name]), ]; sort_y <- y[order(X[, spl_name])]\n  \n  # get X and y for the first group of samples above the best split value\n  g1_y <- sort_y[sort_X[, spl_name] > spl_val]\n  g1_X <- sort_X[sort_X[, spl_name] > spl_val,]\n  \n  # get X and y for the second group of samples below the best split value\n  g2_y <- sort_y[sort_X[, spl_name] < spl_val]\n  g2_X <- sort_X[sort_X[, spl_name] < spl_val,]\n  \n  # return new data (subset of X and y) for a group with Gini index > 0\n  if(gini(g1_y) > 0){return(list(new_X = g1_X, new_y = g1_y))}\n  else if(gini(g2_y) > 0){return(list(new_X = g2_X, new_y = g2_y))}\n  else{return(0)}\n}\nget_new_data(X, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$new_X\n  height weight\n4    171     67\n3    178     85\n1    183     78\n\n$new_y\n[1] Female Male   Female\nLevels: Female Male\n```\n\n\n:::\n:::\n\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.fragment}\n- We can train a decision tree of max_depth = 2\n:::\n\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\ndecision_tree <- function(X, y, max_depth = 2)\n{\n  new_X <- X; new_y <- y\n  df <- data.frame(matrix(ncol = 5, nrow = max_depth))\n  colnames(df) <- c(\"spl_num\", \"spl_name\", \"sign\", \"spl_val\", \"label\")\n  for(i in 1:max_depth)\n  {\n    best_split_output <- get_best_split(new_X, new_y)\n    sex <- unlist(strsplit(best_split_output$sex,\": \"))\n    df[i, \"spl_num\"] <- i\n    df[i, \"spl_name\"] <- best_split_output$spl_name\n    df[i, \"sign\"] <- ifelse(sex[1] == \"Below\", \"<\", \">\")\n    df[i, \"spl_val\"] <- best_split_output$spl_value\n    df[i, \"label\"] <- sex[2]\n    \n    new_data_output <- get_new_data(new_X, new_y)\n    if(length(new_data_output) != 1)\n    {\n      new_X <- new_data_output$new_X\n      new_y <- new_data_output$new_y\n    }\n    else\n    {\n      print(\"All terminal nodes have perfect purity\")\n      break\n    }\n  }\n  return(df)\n}\ndecision_tree(X, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"All terminal nodes have perfect purity\"\n```\n\n\n:::\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| spl_num|spl_name |sign | spl_val|label |\n|-------:|:--------|:----|-------:|:-----|\n|       1|height   |<    |   169.0|Male  |\n|       2|weight   |>    |    81.5|Male  |\n\n</div>\n:::\n:::\n\n:::\n\n:::\n\n::::\n\n## Decision tree from scratch in R: prediction\n\n:::: {.columns}\n\n::: {.column width=\"52%\"}\n\n- Finally, after we have trained the decision tree, we can try to make predictions, and check whether we can reconstruct the labels of the data points\n\n::: {.fragment}\n\n\n::: {.cell .smaller}\n\n```{.r .smaller .cell-code}\npredict_decision_tree <- function(X, y)\n{\n  # Train a decision tree\n  t <- decision_tree(X, y, max_depth = 2)\n  \n  # Parse the output of decision tree and code it via if, else if and else\n  pred_labs <- vector()\n  for(i in 1:dim(X)[1])\n  {\n    if(eval(parse(text=paste0(X[i,t$spl_name[1]],t$sign[1],t$spl_val[1]))))\n    {\n      pred_labs[i] <- t$label[1]\n    }\n    else if(eval(parse(text=paste0(X[i,t$spl_name[2]],t$sign[2],t$spl_val[2]))))\n    {\n      pred_labs[i] <- t$label[2]\n    }\n    else{pred_labs[i] <- ifelse(t$label[2] == \"Male\", \"Female\", \"Male\")}\n  }\n\n  return(cbind(cbind(X, y), pred_labs))\n}\npredict_decision_tree(X, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"All terminal nodes have perfect purity\"\n```\n\n\n:::\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| height| weight|y      |pred_labs |\n|------:|------:|:------|:---------|\n|    183|     78|Female |Female    |\n|    167|     73|Male   |Male      |\n|    178|     85|Male   |Male      |\n|    171|     67|Female |Female    |\n\n</div>\n:::\n:::\n\n:::\n\n:::\n\n::: {.column width=\"48%\"}\n\n::: {.fragment}\n\n- **Random Forest** has two key differences:\n\n  - train multiple decision trees (**bagging**)\n\n  - train trees on **fractions** of input features\n\n![](assets/Bagging.png){width=\"100%\"}\n\n:::\n\n:::\n\n::::\n\n## {background-image=\"/assets/images/cover.jpg\"}\n\n### Thank you! Questions?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n         _                  \nplatform x86_64-pc-linux-gnu\nos       linux-gnu          \nmajor    4                  \nminor    3.2                \n```\n\n\n:::\n:::\n\n\n[{{< meta current_year >}} • [SciLifeLab](https://www.scilifelab.se/) • [NBIS](https://nbis.se/) • [RaukR](https://nbisweden.github.io/raukr-2024)]{.smaller}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}